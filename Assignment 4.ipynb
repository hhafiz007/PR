{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2a9813d1",
      "metadata": {
        "id": "2a9813d1"
      },
      "source": [
        "# Assignment-4 (Perceptron, SVM, and Neural Networks)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d13c8e6",
      "metadata": {
        "id": "7d13c8e6"
      },
      "source": [
        "This part of the assignment shall require you to code the perceptron classifier from the ground up. The perceptron classifier takes uses the weighted sum of input features, uses a threshold to classify between two classes. Usually these classes are -1 and 1 while the threshold is 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "45f0421a",
      "metadata": {
        "id": "45f0421a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JmMLz83F5rja",
      "metadata": {
        "id": "JmMLz83F5rja"
      },
      "source": [
        "# 1. Prepare the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "2IBreuJnmsvB",
      "metadata": {
        "id": "2IBreuJnmsvB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcd114c2-fbc8-4ae0-b589-17df299f2836"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11\n"
          ]
        }
      ],
      "source": [
        "# import the assigned dataset\n",
        "columns = [\"fLength\",\"fWidth\",\"fSize\",\"fConc\",\"fConc1\",\"fAsym\",\"fM3Long\",\"fM3Trans\",\"fAlpha\",\"fDist\",\"targets\"]\n",
        "print(len(columns))\n",
        "\n",
        "df = pd.read_csv(\n",
        "    filepath_or_buffer='https://archive.ics.uci.edu/ml/machine-learning-databases/magic/magic04.data', \n",
        "    header=None, \n",
        "    sep=',')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "D6LXkVd1mtJh",
      "metadata": {
        "id": "D6LXkVd1mtJh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2afe4801-26ed-48b9-c7c4-45f4174136fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-b8b2152667e3>:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[\"targets\"]= df[\"targets\"].astype('category').cat.codes\n",
            "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py:4906: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  return super().drop(\n"
          ]
        }
      ],
      "source": [
        "# Preprocess the dataset with normalization and label encoding\n",
        "df.columns = columns\n",
        "df.dropna(inplace=True,how=\"all\")\n",
        "df = df[[\"fLength\",\"fWidth\",\"fSize\",\"fConc\",\"fConc1\",\"fAsym\",\"fM3Long\",\"fM3Trans\",\"targets\"]]\n",
        "df[\"targets\"]= df[\"targets\"].astype('category').cat.codes\n",
        "df_0 = df[df['targets'].apply(lambda x:x==0)].head(500)\n",
        "df_1 = df[df['targets'].apply(lambda x:x==1)].head(500)\n",
        "df_final = [df_0,df_1]\n",
        "pd.concat(df_final)\n",
        "\n",
        "# df= df[:350]+df[18950:]\n",
        "x = df\n",
        "y = df[\"targets\"]\n",
        "x.drop(\"targets\",axis=1,inplace=True)\n",
        "x =x.values\n",
        "y = y.values\n",
        "y=y.reshape(len(y),1)\n",
        "X_norm = StandardScaler().fit_transform(x)\n",
        "\n",
        "dataset = np.hstack((X_norm, y))\n",
        "dataset = dataset.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "LIwB8dmYq9iH",
      "metadata": {
        "id": "LIwB8dmYq9iH"
      },
      "outputs": [],
      "source": [
        "# Make a train-test split with 80% to training and 20% to testing.\n",
        "x_full_train,x_full_test,y_full_train,y_full_test = train_test_split(X_norm,y, test_size=0.01, random_state=None, shuffle=True,stratify=y)\n",
        "\n",
        "\n",
        "y_full_train = y_full_train.reshape(len(y_full_train),)\n",
        "y_full_test = y_full_test.reshape(len(y_full_test),)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2g0SCXkcmtML",
      "metadata": {
        "id": "2g0SCXkcmtML"
      },
      "outputs": [],
      "source": [
        "# Normalize numerical features and encode the categorical features (if any)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "936fab42",
      "metadata": {
        "id": "936fab42"
      },
      "source": [
        "As this is a classification problem, the target variables must be categorical. Using the feature and target variable information, preprocess the dataset to use given features.\n",
        "\n",
        "This step might require students to scale the features, one-hot encode categorical FEATURES (if any).\n",
        "\n",
        "\n",
        "\n",
        "For feature scaling and one-hot encoding, go through:\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a204d3db",
      "metadata": {
        "id": "a204d3db"
      },
      "source": [
        "# 2: Creating a perceptron model for the processed dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0014b24",
      "metadata": {
        "id": "b0014b24"
      },
      "source": [
        "The perceptron.py file in the resources provides functions to code the perceptron model from scratch.\n",
        "\n",
        "Using the file as reference, write the functions:\n",
        "\n",
        "1. cross_validation_split\n",
        "2. accuracy_metric\n",
        "3. evaluate_algorithm\n",
        "4. predict\n",
        "5. train_weights\n",
        "6. perceptron\n",
        "\n",
        "This step is aimed at providing a comprehensive understanding of the internal functioning of a perceptron model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "b7429d7c",
      "metadata": {
        "id": "b7429d7c"
      },
      "outputs": [],
      "source": [
        "# your code for step 2\n",
        "from random import seed\n",
        "from random import randrange\n",
        "from csv import reader\n",
        "\n",
        "\n",
        "def str_column_to_float(dataset, column):\n",
        "\tfor row in dataset:\n",
        "\t\trow[column] = float(row[column].strip())\n",
        "\n",
        "def str_column_to_int(dataset, column):\n",
        "\tclass_values = [row[column] for row in dataset]\n",
        "\tunique = set(class_values)\n",
        "\tlookup = dict()\n",
        "\tfor i, value in enumerate(unique):\n",
        "\t\tlookup[value] = i\n",
        "\tfor row in dataset:\n",
        "\t\trow[column] = lookup[row[column]]\n",
        "\treturn lookup\n",
        "\n",
        "def cross_validation_split(dataset, n_folds):\n",
        "\tdataset_split = list()\n",
        "\tdataset_copy = list(dataset)\n",
        "\tfold_size = int(len(dataset) / n_folds)\n",
        "\tfor i in range(n_folds):\n",
        "\t\tfold = list()\n",
        "\t\twhile len(fold) < fold_size:\n",
        "\t\t\tindex = randrange(len(dataset_copy))\n",
        "\t\t\tfold.append(dataset_copy.pop(index))\n",
        "\t\tdataset_split.append(fold)\n",
        "\treturn dataset_split\n",
        "\n",
        "\n",
        "def accuracy_metric(actual, predicted):\n",
        "\tcorrect = 0\n",
        "\tfor i in range(len(actual)):\n",
        "\t\tif actual[i] == predicted[i]:\n",
        "\t\t\tcorrect += 1\n",
        "\treturn correct / float(len(actual)) * 100.0\n",
        "\n",
        "\n",
        "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
        "\tfolds = cross_validation_split(dataset, n_folds)\n",
        "\tscores = list()\n",
        "\tfor fold in folds:\n",
        "\t\ttrain_set = list(folds)\n",
        "\t\ttrain_set.remove(fold)\n",
        "\t\ttrain_set = sum(train_set, [])\n",
        "\t\ttest_set = list()\n",
        "\t\tfor row in fold:\n",
        "\t\t\trow_copy = list(row)\n",
        "\t\t\ttest_set.append(row_copy)\n",
        "\t\t\trow_copy[-1] = None\n",
        "\t\tpredicted = algorithm(train_set, test_set, *args)\n",
        "\t\tactual = [row[-1] for row in fold]\n",
        "\t\taccuracy = accuracy_metric(actual, predicted)\n",
        "\t\tscores.append(accuracy)\n",
        "\treturn scores\n",
        "\n",
        "def predict(row, weights):\n",
        "\tactivation = weights[0]\n",
        "\tfor i in range(len(row)-1):\n",
        "\t\tactivation += weights[i + 1] * row[i]\n",
        "\treturn 1.0 if activation >= 0.0 else 0.0\n",
        "\n",
        "def train_weights(train, l_rate, n_epoch):\n",
        "\tweights = [0.0 for i in range(len(train[0]))]\n",
        "\tfor epoch in range(n_epoch):\n",
        "\t\tfor row in train:\n",
        "\t\t\tprediction = predict(row, weights)\n",
        "\t\t\terror = row[-1] - prediction\n",
        "\t\t\tweights[0] = weights[0] + l_rate * error\n",
        "\t\t\tfor i in range(len(row)-1):\n",
        "\t\t\t\tweights[i + 1] = weights[i + 1] + l_rate * error * row[i]\n",
        "\treturn weights\n",
        "\n",
        "def perceptron(train, test, l_rate, n_epoch):\n",
        "\tpredictions = list()\n",
        "\tweights = train_weights(train, l_rate, n_epoch)\n",
        "\tfor row in test:\n",
        "\t\tprediction = predict(row, weights)\n",
        "\t\tpredictions.append(prediction)\n",
        "\treturn(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "808989bc",
      "metadata": {
        "id": "808989bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "803bdcdd-90e2-4ddc-b2d3-826d277e89dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scores: [63.23343848580441, 47.85488958990536, 67.36593059936908]\n",
            "Mean Accuracy: 59.485%\n"
          ]
        }
      ],
      "source": [
        "# your code for step 2\n",
        "def perceptron_call(n_fold):\n",
        "  l_rate = 0.01\n",
        "  n_epoch = 100\n",
        "  scores = evaluate_algorithm(dataset, perceptron, n_fold, l_rate, n_epoch)\n",
        "  print('Scores: %s' % scores)\n",
        "  print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))\n",
        "\n",
        "perceptron_call(3) "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5eb75836",
      "metadata": {
        "id": "5eb75836"
      },
      "source": [
        "# (Bonus) Perceptron model with relaxation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "751fbd6a",
      "metadata": {
        "id": "751fbd6a"
      },
      "source": [
        "Using Relaxation (the descent theorem), compare the performance of perceptron model with and without relaxation (refer class lectures, slides for details on relaxation).\n",
        "\n",
        "Make modifications to the loss function in the perceptron model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "951a9657",
      "metadata": {
        "id": "951a9657"
      },
      "outputs": [],
      "source": [
        "# code for relaxation for the perceptron model\n",
        "\n",
        "def trainRelaxation(train, l_rate, n_epoch):\n",
        "\tweights = [0.0 for i in range(len(train[0]))]\n",
        "\tfor epoch in range(n_epoch):\n",
        "\t\tfor row in train:\n",
        "\t\t\tprediction = predict(row, weights)\n",
        "\t\t\terror = row[-1] - prediction\n",
        "\t\t\tactivation = weights[0]\n",
        "\t\t\tfor i in range(len(row)-1):\n",
        "\t\t\t\tactivation += weights[i + 1] * row[i]\n",
        "\t\t\tweights[0] = weights[0] + l_rate * 2 * activation * error\n",
        "\t\t\tfor i in range(len(row)-1):\n",
        "\t\t\t\tweights[i + 1] = weights[i + 1] + l_rate * 2 * activation  * error * row[i]\n",
        "\treturn weights\n",
        "\n",
        "def perceptronRelaxation(train, test, l_rate, n_epoch):\n",
        "\tpredictions = list()\n",
        "\tweights = trainRelaxation(train, l_rate, n_epoch)\n",
        "\tfor row in test:\n",
        "\t\tprediction = predict(row, weights)\n",
        "\t\tpredictions.append(prediction)\n",
        "\treturn(predictions)  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_fold = 3\n",
        "l_rate = 0.01\n",
        "n_epoch = 100\n",
        "scores = evaluate_algorithm(dataset, perceptronRelaxation, n_fold, l_rate, n_epoch)\n",
        "print('Scores: %s' % scores)\n",
        "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEdHblugAEhA",
        "outputId": "7c8cc399-4e42-4cac-c24f-19080ef17778"
      },
      "id": "JEdHblugAEhA",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scores: [35.61514195583596, 34.81072555205048, 35.063091482649845]\n",
            "Mean Accuracy: 35.163%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95b79cbb",
      "metadata": {
        "id": "95b79cbb"
      },
      "source": [
        "# 3: Batch size for perceptron model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d27d5b9",
      "metadata": {
        "id": "6d27d5b9"
      },
      "source": [
        "Experiment with different batch sizes in the perceptron model (eg: 1, 4, 8).\n",
        "\n",
        "Report (with figures) the difference in performance when using different batch sizes. Inferences without plots might not be awarded points.\n",
        "\n",
        "Report the accuracies for various combinations of batch sizes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "65f8b124",
      "metadata": {
        "id": "65f8b124",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5098c1c6-9a61-41d9-fd2d-9e59cfecd799"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scores: [60.56782334384858, 69.94742376445846]\n",
            "Mean Accuracy: 65.258%\n"
          ]
        }
      ],
      "source": [
        "# code for step 3\n",
        "perceptron_call(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "6572ca2f",
      "metadata": {
        "id": "6572ca2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c3d10b6-a59a-498b-aba7-86c75ca4e6b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scores: [52.008412197686646, 49.50578338590957, 53.14405888538381, 60.1472134595163]\n",
            "Mean Accuracy: 53.701%\n"
          ]
        }
      ],
      "source": [
        "# code for step 3\n",
        "perceptron_call(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "VBCOjJts5QvA",
      "metadata": {
        "id": "VBCOjJts5QvA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b01d812-0dcc-45ee-92f7-a32740f7b486"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scores: [62.56572029442692, 66.19348054679286, 66.40378548895899, 66.92954784437434, 51.34069400630915]\n",
            "Mean Accuracy: 62.687%\n"
          ]
        }
      ],
      "source": [
        "# code for step 3\n",
        "perceptron_call(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wSYUKCg8neAH",
      "metadata": {
        "id": "wSYUKCg8neAH"
      },
      "source": [
        "# 4. SVM's\n",
        "\n",
        "### **Note : You are allowed to use sklearn's SVC classifier for steps 4.1 through 4.3**\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dESPXbsWuF26",
      "metadata": {
        "id": "dESPXbsWuF26"
      },
      "source": [
        "# 4.1 Linear SVM\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Step 1: Implement a linear SVM model to classify the data points. (Look into the 'kernel' parameter)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "GPZt7zZCmtPK",
      "metadata": {
        "id": "GPZt7zZCmtPK"
      },
      "outputs": [],
      "source": [
        "# linear SVM\n",
        "svm_lin_clf = SVC(kernel=\"linear\", C=1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wDG07yCYt1B0",
      "metadata": {
        "id": "wDG07yCYt1B0"
      },
      "source": [
        "# Step 2: Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "xRGroMUFt1J5",
      "metadata": {
        "id": "xRGroMUFt1J5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fab79a4c-70cc-4779-8fde-11146e4cb0b7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(kernel='linear')"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "# training - linear SVM\n",
        "svm_lin_clf.fit(x_full_train, y_full_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2aXe9XNrkDM",
      "metadata": {
        "id": "c2aXe9XNrkDM"
      },
      "source": [
        "# Step 3: Predict for the test points using the model trained in the previous step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "18yLtskurkMR",
      "metadata": {
        "id": "18yLtskurkMR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a4c777d-d1eb-472b-d5a2-8b77ef504631"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
            " 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1\n",
            " 1 0 0 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "# predict - linear SVM\n",
        "svm_lin_acc = svm_lin_clf.score(x_full_test, y_full_test) * 100\n",
        "svm_lin_predict = svm_lin_clf.predict(x_full_test)\n",
        "print(svm_lin_predict)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RoR12SgBpzja",
      "metadata": {
        "id": "RoR12SgBpzja"
      },
      "source": [
        "# 4.2 Kernel SVM - Polynomial kernel\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "# Step 1: Implement a kernel SVM model with a polynomial kernel to classify the data points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "lyDOfuYXp7dG",
      "metadata": {
        "id": "lyDOfuYXp7dG"
      },
      "outputs": [],
      "source": [
        "# kernel SVM - polynomial kernel\n",
        "svm_poly_clf = SVC(kernel=\"poly\", coef0=1, C=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GpP79PDauc2p",
      "metadata": {
        "id": "GpP79PDauc2p"
      },
      "source": [
        "# Step 2: Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "uS6lHWUhuc_X",
      "metadata": {
        "id": "uS6lHWUhuc_X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91009846-db5c-4162-88d0-afc1d1749272"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=3, coef0=1, kernel='poly')"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "# # training - kernel SVM\n",
        "svm_poly_clf.fit(x_full_train, y_full_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7YExuG0I-bt",
        "outputId": "187dd7b6-cbdd-41c8-dafc-b93d1a08c620"
      },
      "id": "X7YExuG0I-bt",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5eGEDVYRr7hz",
      "metadata": {
        "id": "5eGEDVYRr7hz"
      },
      "source": [
        "# Step 3: Predict for the test points using the model trained in the previous step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "c2CS_FKsr7pF",
      "metadata": {
        "id": "c2CS_FKsr7pF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95ba9023-7829-417c-8bb4-a22babd05ae9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 0 0\n",
            " 0 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0\n",
            " 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 1\n",
            " 1 0 0 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "# predict - kernel SVM\n",
        "svm_poly_acc = svm_poly_clf.score(x_full_test, y_full_test) * 100\n",
        "svm_poly_predict = svm_poly_clf.predict(x_full_test)\n",
        "print(svm_poly_predict)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BsKcw44Vp7sd",
      "metadata": {
        "id": "BsKcw44Vp7sd"
      },
      "source": [
        "# 4.3 Kernel SVM - Gaussian kernel\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "# Step 1: Implement a kernel SVM model with a gaussian (Radian Basis function) kernel to classify the data points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "4nLNaT5SqR58",
      "metadata": {
        "id": "4nLNaT5SqR58"
      },
      "outputs": [],
      "source": [
        "# kernel SVM - gaussian kernel\n",
        "svm_rbf_clf = SVC(kernel=\"rbf\", C=1.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QO82alm_unrk",
      "metadata": {
        "id": "QO82alm_unrk"
      },
      "source": [
        "# Step 2: Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "XRlivJbkunys",
      "metadata": {
        "id": "XRlivJbkunys",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f88f5cc-ad59-4425-98b4-80f59800ce33"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=1.5)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "# training - kernel SVM\n",
        "svm_rbf_clf.fit(x_full_train, y_full_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YCEodCuXr_oi",
      "metadata": {
        "id": "YCEodCuXr_oi"
      },
      "source": [
        "# Step 3: Predict for the test points using the model trained in the previous step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "euFQqnXrr_vz",
      "metadata": {
        "id": "euFQqnXrr_vz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63980d33-41e5-468a-8ab3-daf853e3a682"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1\n",
            " 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 0 0\n",
            " 0 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0\n",
            " 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 1\n",
            " 1 0 0 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "# predict - kernel SVM\n",
        "svm_rbf_acc = svm_rbf_clf.score(x_full_test, y_full_test) * 100\n",
        "svm_rbf_predict = svm_rbf_clf.predict(x_full_test)\n",
        "print(svm_rbf_predict)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "D4rej5nCsE3e",
      "metadata": {
        "id": "D4rej5nCsE3e"
      },
      "source": [
        "# 4.4 - Evaluation\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Take the results from each predict step under sections 4.1, 4.2 and 4.3. Consider accuracy as the evaulation metric. Print the accuracies for each of the 3 SVM models.\n",
        "\n",
        "Note: Do not use functions from sklearn.metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "huEssd0Cs4Su",
      "metadata": {
        "id": "huEssd0Cs4Su",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf9760ce-b40e-47da-eb55-b33ddda22b56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracies of the above classifiers are as follows: \n",
            "Accuracy for linear kernel:  75.39267015706807\n",
            "Accuracy for polynomial kernel:  78.53403141361257\n",
            "Accuracy for Gaussian kernel:  78.53403141361257\n"
          ]
        }
      ],
      "source": [
        "print(\"The accuracies of the above classifiers are as follows: \")\n",
        "print(\"Accuracy for linear kernel: \", svm_lin_acc)\n",
        "print(\"Accuracy for polynomial kernel: \", svm_poly_acc)\n",
        "print(\"Accuracy for Gaussian kernel: \", svm_rbf_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Mbb7piNhvmz7",
      "metadata": {
        "id": "Mbb7piNhvmz7"
      },
      "outputs": [],
      "source": [
        "# space for any imports for the following steps"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yspPihkdtYzW",
      "metadata": {
        "id": "yspPihkdtYzW"
      },
      "source": [
        "# 5. Neural Networks\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DORVXa96vcl9",
      "metadata": {
        "id": "DORVXa96vcl9"
      },
      "source": [
        "# 5.1 Single layer neural network\n",
        "\n",
        "You can use either PyTorch or Tensorflow for the implementation\n",
        "\n",
        "---\n",
        "\n",
        "# Step 1: Implement a single layer neural network to classify the data points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "Eekz2V1OvWq8",
      "metadata": {
        "id": "Eekz2V1OvWq8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ee1f8b7-604a-4acb-f8fb-cc58ccafce5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ],
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "\n",
        "def logit(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def derivative(f, z, eps=0.000001):\n",
        "    return (f(z + eps) - f(z - eps))/(2 * eps)\n",
        "def heaviside(z):\n",
        "    return (z >= 0).astype(z.dtype)\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1/(1+np.exp(-z))\n",
        "\n",
        "def mlp_xor(x1, x2, activation=heaviside):\n",
        "    return activation(-activation(x1 + x2 - 1.5) + activation(x1 + x2 - 0.5) - 0.5)\n",
        "\n",
        "def reset_graph(seed=42):\n",
        "    tf.reset_default_graph()\n",
        "    tf.set_random_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "n_inputs = 8 \n",
        "n_hidden1 = 300\n",
        "n_outputs = 2\n",
        "\n",
        "X = tf.compat.v1.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "y = tf.compat.v1.placeholder(tf.int32, shape=(None), name=\"y\")\n",
        "\n",
        "def neuron_layer(X, n_neurons, name, activation=None):\n",
        "    with tf.name_scope(name):\n",
        "        n_inputs = int(X.get_shape()[1])\n",
        "        stddev = 2 / np.sqrt(n_inputs)\n",
        "        init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev)\n",
        "        W = tf.Variable(init, name=\"kernel\")\n",
        "        b = tf.Variable(tf.zeros([n_neurons]), name=\"bias\")\n",
        "        Z = tf.matmul(X, W) + b\n",
        "        if activation is not None:\n",
        "            return activation(Z)\n",
        "        else:\n",
        "            return Z\n",
        "\n",
        "with tf.name_scope(\"dnn1\"):\n",
        "    hidden1 = neuron_layer(X, n_hidden1, name=\"hidden1\",\n",
        "                           activation=tf.nn.relu)\n",
        "    logits = neuron_layer(hidden1, n_outputs, name=\"outputs\")\n",
        "\n",
        "with tf.name_scope(\"loss1\"):\n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
        "                                                              logits=logits)\n",
        "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
        "\n",
        "learning_rate = 0.01\n",
        "\n",
        "with tf.name_scope(\"train1\"):\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "    training_op = optimizer.minimize(loss)\n",
        "\n",
        "with tf.name_scope(\"eval1\"):\n",
        "    correct = tf.nn.in_top_k(logits, y, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "\n",
        "n_epochs = 40\n",
        "batch_size = 50\n",
        "\n",
        "def shuffle_batch(X, y, batch_size):\n",
        "    rnd_idx = np.random.permutation(len(X))\n",
        "    n_batches = len(X) // batch_size\n",
        "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
        "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
        "        yield X_batch, y_batch\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9V0qytGdvYTx",
      "metadata": {
        "id": "9V0qytGdvYTx"
      },
      "source": [
        "# Step 2: Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "y6EOwJ70vYaD",
      "metadata": {
        "id": "y6EOwJ70vYaD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a86ba56f-764c-42ad-f292-9b33d2f2905c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 Batch accuracy: 0.8\n",
            "1 Batch accuracy: 0.82\n",
            "2 Batch accuracy: 0.86\n",
            "3 Batch accuracy: 0.84\n",
            "4 Batch accuracy: 0.78\n",
            "5 Batch accuracy: 0.82\n",
            "6 Batch accuracy: 0.9\n",
            "7 Batch accuracy: 0.8\n",
            "8 Batch accuracy: 0.76\n",
            "9 Batch accuracy: 0.66\n",
            "10 Batch accuracy: 0.84\n",
            "11 Batch accuracy: 0.84\n",
            "12 Batch accuracy: 0.74\n",
            "13 Batch accuracy: 0.78\n",
            "14 Batch accuracy: 0.78\n",
            "15 Batch accuracy: 0.68\n",
            "16 Batch accuracy: 0.84\n",
            "17 Batch accuracy: 0.88\n",
            "18 Batch accuracy: 0.9\n",
            "19 Batch accuracy: 0.82\n",
            "20 Batch accuracy: 0.84\n",
            "21 Batch accuracy: 0.9\n",
            "22 Batch accuracy: 0.84\n",
            "23 Batch accuracy: 0.72\n",
            "24 Batch accuracy: 0.88\n",
            "25 Batch accuracy: 0.84\n",
            "26 Batch accuracy: 0.86\n",
            "27 Batch accuracy: 0.86\n",
            "28 Batch accuracy: 0.86\n",
            "29 Batch accuracy: 0.8\n",
            "30 Batch accuracy: 0.78\n",
            "31 Batch accuracy: 0.9\n",
            "32 Batch accuracy: 0.88\n",
            "33 Batch accuracy: 0.86\n",
            "34 Batch accuracy: 0.82\n",
            "35 Batch accuracy: 0.8\n",
            "36 Batch accuracy: 0.76\n",
            "37 Batch accuracy: 0.78\n",
            "38 Batch accuracy: 0.86\n",
            "39 Batch accuracy: 0.64\n"
          ]
        }
      ],
      "source": [
        "with tf.Session() as sess1:\n",
        "    init.run()\n",
        "    for epoch in range(n_epochs):\n",
        "        for X_batch, y_batch in shuffle_batch(x_full_train, y_full_train, batch_size):\n",
        "            sess1.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "        acc_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
        "        print(epoch, \"Batch accuracy:\", acc_batch)\n",
        "\n",
        "    save_path = saver.save(sess1, \"./my_model1.ckpt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ozkKUCzvvY7b",
      "metadata": {
        "id": "ozkKUCzvvY7b"
      },
      "source": [
        "# Step 3: Predict for the test points using the model trained in the previous step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "EbjD6UXgvZBs",
      "metadata": {
        "id": "EbjD6UXgvZBs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cec5390-54d7-4ed2-ef26-ad4d98c05fe8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted classes: [1 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0]\n",
            "Actual classes:    [0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 1 1]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7486910994764397"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "with tf.Session() as sess1:\n",
        "    saver.restore(sess1, \"./my_model1.ckpt\") # or better, use save_path\n",
        "    Z = logits.eval(feed_dict={X: x_full_test})\n",
        "    y_pred = np.argmax(Z, axis=1)\n",
        "\n",
        "print(\"Predicted classes:\", y_pred[:20])\n",
        "print(\"Actual classes:   \", y_full_test[:20])  \n",
        "\n",
        "correct= 0\n",
        "for i in range(len(y_pred)):\n",
        "  if y_pred[i] == y_full_test[i]:\n",
        "        correct += 1\n",
        "accuracy1 = correct/len(y_pred)\n",
        "accuracy1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pT2B094yv4o_",
      "metadata": {
        "id": "pT2B094yv4o_"
      },
      "source": [
        "# 5.2 Multi - Layer neural network\n",
        "\n",
        "---\n",
        "\n",
        "# Step 1: Implement a multi - layer neural network to classify the data points\n",
        "\n",
        "Additional note: use methods to avoid overfitting appropriately"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "1-IqXvAiv48P",
      "metadata": {
        "id": "1-IqXvAiv48P"
      },
      "outputs": [],
      "source": [
        "n_inputs = 8  \n",
        "n_hidden1 = 300\n",
        "n_hidden2 = 100\n",
        "n_outputs = 2\n",
        "\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
        "\n",
        "with tf.name_scope(\"dnn2\"):\n",
        "    hidden1 = neuron_layer(X, n_hidden1, name=\"hidden1\",\n",
        "                           activation=tf.nn.relu)\n",
        "    hidden2 = neuron_layer(hidden1, n_hidden2, name=\"hidden2\",\n",
        "                           activation=tf.nn.relu)\n",
        "    logits = neuron_layer(hidden2, n_outputs, name=\"outputs\")\n",
        "\n",
        "with tf.name_scope(\"loss2\"):\n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
        "                                                              logits=logits)\n",
        "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
        "\n",
        "learning_rate = 0.01\n",
        "\n",
        "with tf.name_scope(\"train2\"):\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "    training_op = optimizer.minimize(loss)\n",
        "\n",
        "with tf.name_scope(\"eval2\"):\n",
        "    correct = tf.nn.in_top_k(logits, y, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "chE47ggUv5Cj",
      "metadata": {
        "id": "chE47ggUv5Cj"
      },
      "source": [
        "# Step 2: Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "qLudrV1Mv5LK",
      "metadata": {
        "id": "qLudrV1Mv5LK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "384a030b-e081-4f77-b37f-d20cad381315"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 Batch accuracy: 0.78\n",
            "1 Batch accuracy: 0.74\n",
            "2 Batch accuracy: 0.8\n",
            "3 Batch accuracy: 0.78\n",
            "4 Batch accuracy: 0.76\n",
            "5 Batch accuracy: 0.88\n",
            "6 Batch accuracy: 0.76\n",
            "7 Batch accuracy: 0.9\n",
            "8 Batch accuracy: 0.74\n",
            "9 Batch accuracy: 0.74\n",
            "10 Batch accuracy: 0.76\n",
            "11 Batch accuracy: 0.78\n",
            "12 Batch accuracy: 0.88\n",
            "13 Batch accuracy: 0.8\n",
            "14 Batch accuracy: 0.74\n",
            "15 Batch accuracy: 0.72\n",
            "16 Batch accuracy: 0.92\n",
            "18 Batch accuracy: 0.84\n",
            "19 Batch accuracy: 0.7\n",
            "20 Batch accuracy: 0.76\n",
            "21 Batch accuracy: 0.8\n",
            "22 Batch accuracy: 0.86\n",
            "23 Batch accuracy: 0.86\n",
            "24 Batch accuracy: 0.76\n",
            "25 Batch accuracy: 0.86\n",
            "26 Batch accuracy: 0.76\n",
            "27 Batch accuracy: 0.84\n",
            "28 Batch accuracy: 0.84\n",
            "29 Batch accuracy: 0.86\n",
            "30 Batch accuracy: 0.7\n",
            "31 Batch accuracy: 0.84\n",
            "32 Batch accuracy: 0.74\n",
            "33 Batch accuracy: 0.82\n",
            "34 Batch accuracy: 0.88\n",
            "35 Batch accuracy: 0.76\n",
            "36 Batch accuracy: 0.86\n",
            "37 Batch accuracy: 0.78\n",
            "38 Batch accuracy: 0.8\n",
            "39 Batch accuracy: 0.84\n"
          ]
        }
      ],
      "source": [
        "with tf.Session() as sess2:\n",
        "    init.run()\n",
        "    for epoch in range(n_epochs):\n",
        "        for X_batch, y_batch in shuffle_batch(x_full_train, y_full_train, batch_size):\n",
        "            sess2.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "        acc_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
        "        print(epoch, \"Batch accuracy:\", acc_batch)\n",
        "\n",
        "    save_path = saver.save(sess2, \"./my_model2.ckpt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-bGzeSaUv5S8",
      "metadata": {
        "id": "-bGzeSaUv5S8"
      },
      "source": [
        "# Step 3: Predict for the test points using the model trained in the previous step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "kggDlhMtv5ad",
      "metadata": {
        "id": "kggDlhMtv5ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebedac33-700e-491f-eba0-4db2f751ba49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted classes: [0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0]\n",
            "Actual classes:    [0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 1 1]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7958115183246073"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "with tf.Session() as sess2:\n",
        "    saver.restore(sess2, \"./my_model2.ckpt\") # or better, use save_path\n",
        "    Z = logits.eval(feed_dict={X: x_full_test})\n",
        "    y_pred = np.argmax(Z, axis=1)\n",
        "\n",
        "print(\"Predicted classes:\", y_pred[:20])\n",
        "print(\"Actual classes:   \", y_full_test[:20])  \n",
        "\n",
        "correct= 0\n",
        "for i in range(len(y_pred)):\n",
        "  if y_pred[i] == y_full_test[i]:\n",
        "        correct += 1\n",
        "accuracy2 = correct/len(y_pred)\n",
        "accuracy2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XT1hpdNiwLlh",
      "metadata": {
        "id": "XT1hpdNiwLlh"
      },
      "source": [
        "# 5.3 Multi - Layer neural network\n",
        "\n",
        "---\n",
        "\n",
        "# Step 1: Implement a multi - layer neural network to classify the data points\n",
        "\n",
        "**Note :** This must have a different network architecture from the model under section 5.2\n",
        "\n",
        "**Additional note:** use methods to avoid overfitting appropriately"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "ldNn2tNVwLsW",
      "metadata": {
        "id": "ldNn2tNVwLsW"
      },
      "outputs": [],
      "source": [
        "n_inputs = 8 \n",
        "n_hidden1 = 300\n",
        "n_hidden2 = 100\n",
        "n_hidden3 = 200\n",
        "n_outputs = 2\n",
        "\n",
        "X = tf.compat.v1.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "y = tf.compat.v1.placeholder(tf.int32, shape=(None), name=\"y\")\n",
        "\n",
        "\n",
        "with tf.name_scope(\"dnn3\"):\n",
        "    hidden1 = neuron_layer(X, n_hidden1, name=\"hidden1\",\n",
        "                           activation=tf.nn.relu)\n",
        "    hidden2 = neuron_layer(hidden1, n_hidden2, name=\"hidden2\",\n",
        "                           activation=tf.nn.sigmoid)\n",
        "    hidden3 = neuron_layer(hidden2, n_hidden3, name=\"hidden3\",\n",
        "                           activation=tf.nn.relu)\n",
        "    logits = neuron_layer(hidden3, n_outputs, name=\"outputs\")\n",
        "\n",
        "with tf.name_scope(\"loss3\"):\n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
        "                                                              logits=logits)\n",
        "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
        "\n",
        "with tf.name_scope(\"train3\"):\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "    training_op = optimizer.minimize(loss)\n",
        "\n",
        "with tf.name_scope(\"eval3\"):\n",
        "    correct = tf.nn.in_top_k(logits, y, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()    \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ntq2avYhwNL8",
      "metadata": {
        "id": "Ntq2avYhwNL8"
      },
      "source": [
        "# Step 2: Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "hu7oU-eVwNSK",
      "metadata": {
        "id": "hu7oU-eVwNSK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07bc6f46-189b-422c-c045-4e952734158d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 Batch accuracy: 0.72\n",
            "1 Batch accuracy: 0.8\n",
            "2 Batch accuracy: 0.78\n",
            "3 Batch accuracy: 0.9\n",
            "4 Batch accuracy: 0.7\n",
            "5 Batch accuracy: 0.74\n",
            "6 Batch accuracy: 0.84\n",
            "7 Batch accuracy: 0.78\n",
            "8 Batch accuracy: 0.82\n",
            "9 Batch accuracy: 0.82\n",
            "10 Batch accuracy: 0.82\n",
            "11 Batch accuracy: 0.78\n",
            "12 Batch accuracy: 0.84\n",
            "13 Batch accuracy: 0.88\n",
            "14 Batch accuracy: 0.8\n",
            "15 Batch accuracy: 0.86\n",
            "16 Batch accuracy: 0.76\n",
            "17 Batch accuracy: 0.82\n",
            "18 Batch accuracy: 0.86\n",
            "19 Batch accuracy: 0.82\n",
            "20 Batch accuracy: 0.8\n",
            "21 Batch accuracy: 0.84\n",
            "22 Batch accuracy: 0.84\n",
            "23 Batch accuracy: 0.84\n",
            "24 Batch accuracy: 0.82\n",
            "25 Batch accuracy: 0.8\n",
            "26 Batch accuracy: 0.88\n",
            "27 Batch accuracy: 0.82\n",
            "28 Batch accuracy: 0.78\n",
            "29 Batch accuracy: 0.76\n",
            "30 Batch accuracy: 0.86\n",
            "31 Batch accuracy: 0.88\n",
            "32 Batch accuracy: 0.86\n",
            "33 Batch accuracy: 0.82\n",
            "34 Batch accuracy: 0.88\n",
            "35 Batch accuracy: 0.74\n",
            "36 Batch accuracy: 0.86\n",
            "37 Batch accuracy: 0.76\n",
            "38 Batch accuracy: 0.82\n",
            "39 Batch accuracy: 0.8\n"
          ]
        }
      ],
      "source": [
        "with tf.Session() as sess3:\n",
        "    init.run()\n",
        "    for epoch in range(n_epochs):\n",
        "        for X_batch, y_batch in shuffle_batch(x_full_train, y_full_train, batch_size):\n",
        "            sess3.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "        acc_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
        "        print(epoch, \"Batch accuracy:\", acc_batch)\n",
        "\n",
        "    save_path = saver.save(sess3, \"./my_model3.ckpt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pZuLLFWGwNYw",
      "metadata": {
        "id": "pZuLLFWGwNYw"
      },
      "source": [
        "# Step 3: Predict for the test points using the model trained in the previous step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "aUlxFb-mwNfg",
      "metadata": {
        "id": "aUlxFb-mwNfg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db00ea61-992a-4eed-8b41-ea45fcf7f5bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted classes: [0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0]\n",
            "Actual classes:    [0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 1 1]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7905759162303665"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "with tf.Session() as sess3:\n",
        "    saver.restore(sess3, \"./my_model3.ckpt\") # or better, use save_path\n",
        "    Z = logits.eval(feed_dict={X: x_full_test})\n",
        "    y_pred = np.argmax(Z, axis=1)\n",
        "\n",
        "print(\"Predicted classes:\", y_pred[:20])\n",
        "print(\"Actual classes:   \", y_full_test[:20]) \n",
        "\n",
        "correct= 0\n",
        "for i in range(len(y_pred)):\n",
        "  if y_pred[i] == y_full_test[i]:\n",
        "        correct += 1\n",
        "accuracy3 = correct/len(y_pred)\n",
        "accuracy3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "i6OBpYaWxxwG",
      "metadata": {
        "id": "i6OBpYaWxxwG"
      },
      "source": [
        "# 5.4 - Evaluation\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Take the results from each predict step under sections 5.1, 5.2 and 5.3. Consider accuracy as the evaulation metric. Print the accuracies for each of the 3 neural network architectures.\n",
        "\n",
        "Note: Do not use functions from sklearn.metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "ry0jxD2Oxwq8",
      "metadata": {
        "id": "ry0jxD2Oxwq8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c9fa9f6-0023-4b87-f06a-e1a7d17975e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy with single layer neural network is 0.7486910994764397\n",
            "The accuracy from two layer neural network is 0.7958115183246073\n",
            "The accuracy from three layer neural network is is 0.7905759162303665\n"
          ]
        }
      ],
      "source": [
        "print(\"The accuracy with single layer neural network is\",accuracy1);\n",
        "print(\"The accuracy from two layer neural network is\",accuracy2);\n",
        "print(\"The accuracy from three layer neural network is is\",accuracy3);\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2n2RmSlmworT",
      "metadata": {
        "id": "2n2RmSlmworT"
      },
      "source": [
        "# 6 - Paragraph questions\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "# Q1 : Briefly explain the methods you used to prevent overfitting for the models under section 5.2 and 5.3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4NNoPoY2wpiA",
      "metadata": {
        "id": "4NNoPoY2wpiA"
      },
      "source": [
        "***space for Q1's answer***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bXgMyWtEwo4K",
      "metadata": {
        "id": "bXgMyWtEwo4K"
      },
      "source": [
        "# Q2: Compare the performances of models under sections 2, 3, 4 and 5 namely perceptron, descent procedure, SVM's and neural networks. List down few points on why you think certain models performed better than others\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "m-612AuiwqWP",
      "metadata": {
        "id": "m-612AuiwqWP"
      },
      "source": [
        "***space for Q2's answer***"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vDwZXk3z5QvY"
      },
      "id": "vDwZXk3z5QvY",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "wSYUKCg8neAH",
        "yspPihkdtYzW",
        "2n2RmSlmworT"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3.10.1 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}